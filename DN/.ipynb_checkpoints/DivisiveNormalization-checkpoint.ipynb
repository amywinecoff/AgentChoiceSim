{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, chi2\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #preallocate image matrices for choices\n",
    "# #This pertains to estimating covariance matrices of the error differences\n",
    "# #See Train book on discrete choice analysis p 113\n",
    "# #\"This matrix can be used to transform the covariance matrix of\n",
    "# #errors into the covariance matrix of error differences: ~Ωi = MiΩMi.T .\n",
    "# temp = np.identity(Jm-1)\n",
    "# M = np.empty((Jm, Jm-1, 12))\n",
    "# for i in range(1, Jm+1):\n",
    "#     M[i-1] = np.concatenate((temp[:,0:i-1], -1*np.ones((Jm-1,1)), temp[:, i-1:]), axis=1)\n",
    "\n",
    "# #Matrices for only the chosen options\n",
    "# Mi=M[y-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DivisiveNormalization(theta, data):\n",
    "    denom = theta[0] + np.multiply(theta[1], np.linalg.norm(data, theta[2], 1))\n",
    "    v=np.divide(data.T, denom)\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPiProbitQuad(Mi, v):\n",
    "    \n",
    "    MiT=np.transpose(Mi, axes=(0,2,1))\n",
    "    T=v.shape[0]\n",
    "    [x, w] = np.polynomial.hermite.hermgauss(100)\n",
    "\n",
    "    #I honestly don't really know how tensordot works, but these lines of code return the correct values\n",
    "    c = np.tensordot(MiT,v, axes=([1,0]))\n",
    "    cT=np.transpose(c, axes=(0,2,1))\n",
    "    vi = cT.diagonal() #This matches vi in MATLAB for s=1, trials 8,10,14\n",
    "    \n",
    "    #first part of equation in ProbaChoice.m, line 242\n",
    "    z1=np.multiply(-2**0.5, vi)\n",
    "\n",
    "    #second part of equation in ProbaChoice.m, line 242\n",
    "    z2=np.multiply(-2**0.5, x)\n",
    "\n",
    "    #These values have been validated\n",
    "    zz = [z1-ele for ele in z2]\n",
    "\n",
    "    aa=np.prod(norm.cdf(zz), axis=1)\n",
    "    #Pi have been validated\n",
    "    Pi=np.divide(np.sum(np.multiply(w.reshape(100,1), aa), axis=0), np.pi**0.5)\n",
    "    \n",
    "    return Pi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 19,  8, 10,  0,  6,  0,  3,  2,  8,  3,  1, 16,  4,  0, 14,  1,\n",
       "       11, 13,  9, 15, 19,  9,  0,  7,  1,  1,  1,  5, 18,  1, 12,  1,  4,\n",
       "       14,  1, 14,  0,  1,  1,  1,  2,  2, 13,  4,  0,  4,  0,  1,  1,  0,\n",
       "       13,  6,  8,  9, 19, 18,  3,  2,  9,  6,  1,  0,  8,  1,  0,  0, 13,\n",
       "        4, 17,  6,  0, 11,  1,  4,  1,  7,  4,  4, 17, 16,  0, 13,  0,  7,\n",
       "       14,  1,  7,  1,  9,  0,  4,  8,  0,  0, 17,  2,  8,  3,  2,  2,  0,\n",
       "        3,  1, 13, 18, 17,  0,  0,  1,  7,  3,  1, 12,  2, 16,  8,  3, 12,\n",
       "        7,  0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=choice_set_vals\n",
    "choices = chosen_vals\n",
    "v=DivisiveNormalization(theta=thetaDN, data=choice_set_vals)\n",
    "probs = np.empty(data.shape)\n",
    "#get the size of the choice array. Choice arrays must be the same size\n",
    "Jm=data.shape[1]\n",
    "temp = np.identity(Jm-1)\n",
    "M = np.empty((Jm, Jm-1, Jm))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(Jm):\n",
    "    M[i] = np.concatenate((temp[:,0:i], -1*np.ones((Jm-1,1)), temp[:, i:]), axis=1)\n",
    "\n",
    "Mi=M[chosen_vals]\n",
    "pi = calcPiProbitQuad(Mi, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from Bollen et al., 2010\n",
    "choice = pd.read_csv('/Users/amywinecoff/Documents/CITP/Research/Github/AgentChoiceSim/co1_wide.csv')  \n",
    "\n",
    "#for now, remove the conditions with 5 options so I can figure out the code for a fixed set size\n",
    "choice = choice[~choice['condition'].isin(['Top5', 'Top5_NR'])]\n",
    "\n",
    "score_cols = [c for c in choice.columns if 'score' in c]\n",
    "movie_cols = [c for c in choice.columns if 'movie' in c]\n",
    "choice_set_vals = np.array(choice[score_cols]/10)\n",
    "\n",
    "choice['chosen_num']=None\n",
    "for idx, m in enumerate(movie_cols):\n",
    "    choice['chosen_num'] = np.where(choice[m]==choice[\"choice\"], idx, choice['chosen_num'])\n",
    "chosen_vals = np.array(choice['chosen_num'].astype(int).values)\n",
    "\n",
    "chosen = choice_set_vals[np.arange(len(choice_set_vals)), chosen_vals]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPiChosen(theta, data, choices):\n",
    "    \n",
    "    v=DivisiveNormalization(theta=thetaDN, data=data)\n",
    "    probs = np.empty(data.shape)\n",
    "    #get the size of the choice array. Choice arrays must be the same size\n",
    "    Jm=data.shape[1]\n",
    "    temp = np.identity(Jm-1)\n",
    "    M = np.empty((Jm, Jm-1, Jm))\n",
    "\n",
    "\n",
    "    for i in range(Jm):\n",
    "        M[i] = np.concatenate((temp[:,0:i], -1*np.ones((Jm-1,1)), temp[:, i:]), axis=1)\n",
    "\n",
    "    Mi=M[choices]\n",
    "    pi = calcPiProbitQuad(Mi, v)\n",
    "    \n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_check = calcPiChosen(theta=thetaDN, data=choice_set_vals, choices=chosen_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPiAll(theta, data):\n",
    "    \n",
    "    v=DivisiveNormalization(theta, data)\n",
    "    \n",
    "    probs = np.empty(data.shape)\n",
    "    #get the size of the choice array. Choice arrays must be the same size\n",
    "    Jm=data.shape[1]\n",
    "    temp = np.identity(Jm-1)\n",
    "    M = np.empty((Jm, Jm-1, Jm))\n",
    "\n",
    "\n",
    "    for i in range(Jm):\n",
    "        M[i] = np.concatenate((temp[:,0:i], -1*np.ones((Jm-1,1)), temp[:, i:]), axis=1)\n",
    "    \n",
    "    for i in range(Jm):\n",
    "        y=np.array([i]*data.shape[0])\n",
    "        #print(y)\n",
    "        \n",
    "        #Matrices for only the chosen options\n",
    "        Mi=M[y]\n",
    "        #print(Mi)\n",
    "        \n",
    "        pi=calcPiProbitQuad(Mi,v)\n",
    "        probs[:,i]=pi.T\n",
    "        #print(\"pi is {}\".format(pi))\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###CONFIRMATORY ANALYSIS TO TEST MATCH WITH WEBB DATA\n",
    "# #create a vector of the WTP values. These values are from data(1).X, cells 8, 10, and 14\n",
    "# d = np.array([\n",
    "#              [4, 2.33, 1.875, 1.8, 1.5, 1.495, 1.335, 1.275, 1.125, 1.09, 1, 0.925],\n",
    "#              [2.125, 2.125, 2.025, 2.0, 1.875, 1.495, 1.485, 1.335, 1.275, 1.075, 1.0, 0.625],\n",
    "#              [4.0, 2.17,  2.0, 2.0, 1.875, 1.875, 1.5, 1.485, 1.335, 1.275, 1.09, 1.075],\n",
    "#              ])\n",
    "\n",
    "# # #These are the chosen options for s=1, on trials 8, 10,14\n",
    "# y=np.array([4,3,2])\n",
    "\n",
    "# # #Choice set size for trials 8,10,14 for subject 1\n",
    "# Jm=12\n",
    "\n",
    "# # #sigma, omega(w), beta\n",
    "# theta = [0.0000, 0.2376, 0.9739]\n",
    "# temp = np.identity(Jm-1)\n",
    "# M = np.empty((Jm, Jm-1, 12))\n",
    "# for i in range(1, Jm+1):\n",
    "#     M[i-1] = np.concatenate((temp[:,0:i-1], -1*np.ones((Jm-1,1)), temp[:, i-1:]), axis=1)\n",
    "\n",
    "# #Matrices for only the chosen options\n",
    "# Mi=M[y-1]\n",
    "\n",
    "# #This result has been spot checked against the values returned by the MATLAB code for data(1).X, cells 8, 10, and 14 \n",
    "# v=DivisiveNormalization(theta=theta, data=d)\n",
    "# pi=calcPiProbitQuad(Mi,v)\n",
    "\n",
    "# print(\"probs for chosen options only: {}\".format(pi))\n",
    "# print(pi) \n",
    "# #[0.08327671 0.10499576 0.09305649]\n",
    "\n",
    "# probs=calcPiAll(theta=theta,data=d)\n",
    "# print(\"probs for all options: {}\".format(probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation of user choices\n",
    "We simulate 100,000 trials of each of the 3 choice sets and use the values yielded by the `DivisiveNormalization` method + a random noise vector and check that the choice probabilities are roughly in line with the analytic probabilities from `calcPiProbitQuad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def chose_item_dn(d, num_it=1000, theta = [0.0000, 0.2376, 0.9739])\n",
    "freq_chosen = np.array([0., 0., 0.])\n",
    "num_it = 100000\n",
    "v = DivisiveNormalization(theta=theta, data=d)\n",
    "# the following covariance matrix has the structure\n",
    "# [ 1    0.5    ...    0.5 ]\n",
    "# [ 0.5    1    ...    0.5 ]\n",
    "# [ 0.5   ...    1    0.5  ]\n",
    "# [ 0.5   0.5   ...    1   ]\n",
    "cov = np.ones((12, 12)) * 0.5\n",
    "cov[np.arange(12), np.arange(12)] = 1\n",
    "mean = np.zeros(12)\n",
    "for i in range(num_it):\n",
    "    eps = np.random.multivariate_normal(mean, cov, size=3).T\n",
    "    u = v + eps\n",
    "    item_chosen = (u.argmax(axis=0) == (y-1)).astype(float)\n",
    "    freq_chosen += item_chosen / num_it\n",
    "    \n",
    "print(freq_chosen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Steps to Computing a Power Analysis Given an Experimental Design and value of theta\n",
    "1. Read in scores into correct np array format\n",
    "2. Chose the item given its normalized value \n",
    "3. Calculate the probability of the chosen item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = choice_set_vals.values\n",
    "#sigma, omega(w), beta\n",
    "#theta_h1 = [0.0000, 0.2376, 0.9739]\n",
    "probs=calcPiAll(theta=t, data=d)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def chose_item_dn(d, theta = [0.0000, 0.2376, 0.9739]):\n",
    "def chose_item(theta, data, return_utility=False):\n",
    "    probs=calcPiAll(theta=t, data=data)\n",
    "    num_subj = data.shape[0]\n",
    "    Jm = data.shape[1]\n",
    "\n",
    "    v = DivisiveNormalization(theta=theta, data=data)\n",
    "    # the following covariance matrix has the structure\n",
    "    # [ 1    0.5    ...    0.5 ]\n",
    "    # [ 0.5    1    ...    0.5 ]\n",
    "    # [ 0.5   ...    1    0.5  ]\n",
    "    # [ 0.5   0.5   ...    1   ]\n",
    "\n",
    "\n",
    "    cov = np.ones((Jm, Jm)) * 0.5\n",
    "    cov[np.arange(Jm), np.arange(Jm)] = 1\n",
    "    mean = np.zeros(Jm)\n",
    "    #for i in range(num_it):\n",
    "    eps = np.random.multivariate_normal(mean, cov, size=num_subj).T\n",
    "    #print(eps)\n",
    "    u = v + eps\n",
    "    item_chosen = u.argmax(axis=0)\n",
    "    \n",
    "    if return_utility:\n",
    "        return item_chosen, u\n",
    "    else:\n",
    "        return item_chosen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcModelLL(data, theta, null_theta=None):\n",
    "    \"\"\"Calculates the log likelikihood given theta values for a DN model. If a null model is being tested,\n",
    "    it will chose the item based on the alternative model, then calculate the probability of that choice, and the \n",
    "    log-likelihood given both the alternative model and the null model\n",
    "    \"\"\"\n",
    "    #This is not really right. Need to figure out how to solve the probability issue since this is calculating based on the theoretical prob, which is not the same as the observeed prob\n",
    "    ##TODO: Fix this so that it works on variable data size. Right now only running on 20-movie decisions\n",
    "    #probably need to calculate this based on the calculated u, not on the theoretical probs\n",
    "    probs=calcPiAll(theta=theta, data=data)\n",
    "    item_chosen = chose_item(theta=theta, data=data) \n",
    "    \n",
    "    \n",
    "    chosen_probs=probs[np.arange(len(probs)), item_chosen]\n",
    "    #add epsilon to all values to prevent divide by zero error\n",
    "    chosen_probs = chosen_probs + sys.float_info.epsilon\n",
    "    LL = sum(np.log(chosen_probs))\n",
    "    \n",
    "    if null_theta:\n",
    "        null_probs=calcPiAll(theta=null_theta, data=data)\n",
    "        #add epsilon to all values to prevent divide by zero error\n",
    "        null_probs = null_probs + sys.float_info.epsilon\n",
    "        #print(\"calculating null theta\")\n",
    "        \n",
    "        null_chosen_probs = null_probs[np.arange(len(null_probs)), item_chosen]\n",
    "        #print(\"null chosen probs {}\".format(null_chosen_probs))\n",
    "        null_LL = sum(np.log(null_chosen_probs))\n",
    "    else:\n",
    "        null_LL = None\n",
    "        #null_chosen_probs = None\n",
    "    \n",
    "    return LL, null_LL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vuong_test(p1, p2):\n",
    "    r\"\"\"\n",
    "    https://gist.github.com/jseabold/6617976\n",
    "    Vuong-test for non-nested models.\n",
    "    Parameters\n",
    "    ----------\n",
    "    p1 : array-like\n",
    "        f1(Y=y_i | x_i)\n",
    "    p2 : array-like\n",
    "        f2(Y=y_i | x_i)\n",
    "    Notes\n",
    "    -----\n",
    "    This is hard-coded for testing Poisson vs. Zero-inflated. E.g.,\n",
    "    it does not account for\n",
    "    Let f_j(y_i|x_i) denote the predicted probability that random variable Y\n",
    "    equals y_i under the assumption that the distribution is f_j(y_i|x_i) for\n",
    "    j = 1,2. Let\n",
    "    .. math::\n",
    "       m_i = log(\\frac{f_1(y_i|x_i)}{f_2(y_i|x_i)})\n",
    "    The test statistic from Vuong to test the hypothesis of Model 1 vs.\n",
    "    Model 2 is\n",
    "    .. math::\n",
    "       v = \\frac{\\sqrt{n}(1/n \\sum_{i=1}^{n}m_i)}{\\sqrt{1/n \\sum_{i=1}^{n}(m_i - \\bar{m})^2}}\n",
    "    This statistic has a limiting standard normal distribution. Values of\n",
    "    v greater than ~2, indicate that model 1 is preferred. Values of V\n",
    "    less than ~-2 indicate the model 2 is preferred. Values of |V| < ~2 are\n",
    "    inconclusive.\n",
    "    References\n",
    "    ----------\n",
    "    Greene, W. Econometric Analysis.\n",
    "    Vuong, Q.H. 1989 \"Likelihood ratio tests for model selection and\n",
    "        non-nested hypotheses.\" Econometrica. 57: 307-333.\n",
    "    \"\"\"\n",
    "    m = np.log(p1) - np.log(p2)\n",
    "    n = len(m)\n",
    "    v = n ** .5 * m.mean() / m.std()\n",
    "    return v, stats.norm.sf(np.abs(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nestedLRT(LL, nullLL):\n",
    "    \n",
    "    df = len([ele for idx, ele in enumerate(thetaDN) if thetaDNNull[idx]!=ele])\n",
    "    LR = 2*(LL-nullLL)\n",
    "    #consider using chi2.sf since sometimes it is more accurate? https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2.html\n",
    "    p=1 - chi2.cdf(LR, df)\n",
    "  \n",
    "    return LR, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnLL_preds = calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calcModelLL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d367b16c161b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mthetaDN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.114\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.177\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#Webb 2020 sigma and omega only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mthetaDNNull\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.114\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#Fix omega to 0 to test hypothesis that normalization occurrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdnLL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnullLL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalcModelLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthetaDN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthetaDNNull\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnestedLRT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnLL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnullLL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'calcModelLL' is not defined"
     ]
    }
   ],
   "source": [
    "thetaDN=[0.114, 0.177, 1]#Webb 2020 sigma and omega only\n",
    "thetaDNNull = [0.114, 0, 1]#Fix omega to 0 to test hypothesis that normalization occurrs\n",
    "dnLL, nullLL, = calcModelLL(d, thetaDN, thetaDNNull)\n",
    "LR, p = nestedLRT(dnLL, nullLL)\n",
    "\n",
    "print(LR, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaDNb=[0.012, 0.412, 25.74]\n",
    "dnb_probs=calcPiAll(theta=thetaDNb, data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LL, nullLL)\n",
    "LR = 2*(LL-nullLL)\n",
    "print(LR)\n",
    "p=1 - chi2.cdf(LR, 2)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = np.array([\n",
    " #            [4, 2.33, 1.875, 1.8, 1.5, 1.495, 1.335, 1.275, 1.125, 1.09, 1, 0.925],              \n",
    "  #           [2.125, 2.125, 2.025, 2.0, 1.875, 1.495, 1.485, 1.335, 1.275, 1.075, 1.0, 0.625],\n",
    "   #          [4.0, 2.17,  2.0, 2.0, 1.875, 1.875, 1.5, 1.485, 1.335, 1.275, 1.09, 1.075],\n",
    "    #        ])\n",
    "#omega allowed to vary. Set to value in Webb et al., 2020\n",
    "theta_h1 = [1.0, 0.117, 1.0]\n",
    "#This is the null model that tests that omega != 0\n",
    "theta_h0 = [theta_h1[0], 0, theta_h1[2]]\n",
    "\n",
    "LLs = calcModelLL(theta=theta_h1, data=d, null_theta=theta_h0)\n",
    "#LL_h1 = calcModelLL(theta=theta_h1, data=d)\n",
    "\n",
    "print(\"LL for H0 model: {}\".format(LLs[1]))\n",
    "print(\"LL for H1 model: {}\".format(LLs[0]))\n",
    "#print(LL_h1)#-362.68216377703664\n",
    "LR = 2*(LLs[0]-LLs[1])\n",
    "print(LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=1 - chi2.cdf(LR, 1)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = choice_set_vals.values\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = choice_set_vals.values /20\n",
    "#omega allowed to vary. Set to value in Webb et al., 2020\n",
    "theta_h1 = [0.44, 0.0006, 1.0]\n",
    "#This is the null model that tests that omega != 0\n",
    "theta_h0 = [theta_h1[0], 0, theta_h1[2]]\n",
    "\n",
    "LLs = calcModelLL(theta=theta_h1, data=d, null_theta=theta_h0)\n",
    "#LL_h1 = calcModelLL(theta=theta_h1, data=d)\n",
    "\n",
    "print(\"LL for H0 model: {}\".format(LLs[1]))\n",
    "print(\"LL for H1 model: {}\".format(LLs[0]))\n",
    "#print(LL_h1)#-362.68216377703664\n",
    "LR = 2*(LLs[0]-LLs[1])\n",
    "print(LR)\n",
    "p=1 - chi2.cdf(LR, 1)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_probs=calcPiAll(theta=theta_h0, data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_probs_df= pd.DataFrame(null_probs)\n",
    "null_probs_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_set_vals.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as Python\n",
    "#!jupyter nbconvert --to script DivisiveNormalization.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
