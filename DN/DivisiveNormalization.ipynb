{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a vector of the WTP values. These values are from data(1).X, cells 8, 10, and 14\n",
    "d = np.array([\n",
    "            [4, 2.33, 1.875, 1.8, 1.5, 1.495, 1.335, 1.275, 1.125, 1.09, 1, 0.925],\n",
    "            [2.125, 2.125, 2.025, 2.0, 1.875, 1.495, 1.485, 1.335, 1.275, 1.075, 1.0, 0.625],\n",
    "            [4.0, 2.17,  2.0, 2.0, 1.875, 1.875, 1.5, 1.485, 1.335, 1.275, 1.09, 1.075],\n",
    "            ])\n",
    "\n",
    "#These are the chosen options for s=1, on trials 8, 10,14\n",
    "y=np.array([4,3,2])\n",
    "\n",
    "#Choice set size for trials 8,10,14 for subject 1\n",
    "Jm=12\n",
    "\n",
    "#sigma, omega(w), beta\n",
    "theta = [0.0000, 0.2376, 0.9739]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preallocate image matrices for choices\n",
    "#This pertains to estimating covariance matrices of the error differences\n",
    "#See Train book on discrete choice analysis p 113\n",
    "#\"This matrix can be used to transform the covariance matrix of\n",
    "#errors into the covariance matrix of error differences: ~Ωi = MiΩMi.T .\n",
    "temp = np.identity(Jm-1)\n",
    "M = np.empty((Jm, Jm-1, 12))\n",
    "for i in range(1, Jm+1):\n",
    "    M[i-1] = np.concatenate((temp[:,0:i-1], -1*np.ones((Jm-1,1)), temp[:, i-1:]), axis=1)\n",
    "\n",
    "#Matrices for only the chosen options\n",
    "Mi=M[y-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DivisiveNormalization(theta, data):\n",
    "    denom = theta[0] + np.multiply(theta[1], np.linalg.norm(data, theta[2], 1))\n",
    "    v=np.divide(data.T, denom)\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPiProbitQuad(Mi, v):\n",
    "    \n",
    "    MiT=np.transpose(Mi, axes=(0,2,1))\n",
    "    T=v.shape[0]\n",
    "    [x, w] = np.polynomial.hermite.hermgauss(100)\n",
    "\n",
    "    #I honestly don't really know how tensordot works, but these lines of code return the correct values\n",
    "    c = np.tensordot(MiT,v, axes=([1,0]))\n",
    "    cT=np.transpose(c, axes=(0,2,1))\n",
    "    vi = cT.diagonal() #This matches vi in MATLAB for s=1, trials 8,10,14\n",
    "    \n",
    "    #first part of equation in ProbaChoice.m, line 242\n",
    "    z1=np.multiply(-2**0.5, vi)\n",
    "\n",
    "    #second part of equation in ProbaChoice.m, line 242\n",
    "    z2=np.multiply(-2**0.5, x)\n",
    "\n",
    "    #These values have been validated\n",
    "    zz = [z1-ele for ele in z2]\n",
    "\n",
    "    aa=np.prod(norm.cdf(zz), axis=1)\n",
    "    #Pi have been validated\n",
    "    Pi=np.divide(np.sum(np.multiply(w.reshape(100,1), aa), axis=0), np.pi**0.5)\n",
    "    \n",
    "    return Pi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This result has been spot checked against the values returned by the MATLAB code for data(1).X, cells 8, 10, and 14 \n",
    "v=DivisiveNormalization(theta=theta, data=d)\n",
    "pi=calcPiProbitQuad(Mi,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08327671 0.10499576 0.09305649]\n"
     ]
    }
   ],
   "source": [
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation of user choices\n",
    "We simulate 100,000 trials of each of the 3 choice sets and use the values yielded by the `DivisiveNormalization` method + a random noise vector and check that the choice probabilities are roughly in line with the analytic probabilities from `calcPiProbitQuad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0817  0.10785 0.09297]\n"
     ]
    }
   ],
   "source": [
    "freq_chosen = np.array([0., 0., 0.])\n",
    "num_it = 100000\n",
    "v = DivisiveNormalization(theta=theta, data=d)\n",
    "# the following covariance matrix has the structure\n",
    "# [ 1    0.5    ...    0.5 ]\n",
    "# [ 0.5    1    ...    0.5 ]\n",
    "# [ 0.5   ...    1    0.5  ]\n",
    "# [ 0.5   0.5   ...    1   ]\n",
    "cov = np.ones((12, 12)) * 0.5\n",
    "cov[np.arange(12), np.arange(12)] = 1\n",
    "mean = np.zeros(12)\n",
    "for i in range(num_it):\n",
    "    eps = np.random.multivariate_normal(mean, cov, size=3).T\n",
    "    u = v + eps\n",
    "    item_chosen = (u.argmax(axis=0) == (y-1)).astype(float)\n",
    "    freq_chosen += item_chosen / num_it\n",
    "    \n",
    "print(freq_chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook DivisiveNormalization.ipynb to script\n",
      "[NbConvertApp] Writing 3472 bytes to DivisiveNormalization.py\n"
     ]
    }
   ],
   "source": [
    "# save as Python\n",
    "!jupyter nbconvert --to script DivisiveNormalization.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t_recs",
   "language": "python",
   "name": "t_recs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
